{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Aivle 스쿨 지원 질문, 답변 챗봇 만들기**\n","# 단계2 : 모델링"],"metadata":{"id":"3xIPZjFU5rjt"}},{"cell_type":"markdown","source":["## 0.미션"],"metadata":{"id":"-FPypzell2uc"}},{"cell_type":"markdown","source":["* 다음 세가지 챗봇을 만들고 비교해 봅시다.\n","* 챗봇1. Word2Vec 임베딩 벡터 기반 머신러닝 분류 모델링\n","    * Word2Vec 모델을 만들고 임베딩 벡터를 생성합니다.\n","    * 임베딩 벡터를 이용하여 intent를 분류하는 모델링을 수행합니다.\n","        * 이때, LightGBM을 추천하지만, 다른 알고리즘을 이용할수 있습니다.\n","    * 예측된 intent의 답변 중 임의의 하나를 선정하여 출력합니다.\n","* 챗봇2. 단계별 모델링1\n","    * 1단계 : type(일상대화 0, 에이블스쿨Q&A 1) 분류 모델 만들기\n","        * Embedding + LSTM 모델링\n","    * 2단계 : 사전학습된 Word2Vec 모델을 로딩하여 train의 임베딩벡터 저장\n","    * 코사인 유사도로 intent 찾아 답변 출력\n","        * 새로운 문장의 임베딩벡터와 train의 임베딩 벡터간의 코사인 유사도 계산\n","        * 가장 유사도가 높은 질문의 intent를 찾아 답변 출력하기\n","* 챗봇3. 단계별 모델링2\n","    * 1단계 : 챗봇2의 1단계 모델을 그대로 활용\n","    * 2단계 : FastText 모델 생성하여 train의 임베딩벡터 저장\n","    * 코사인 유사도로 intent 찾아 답변 출력\n","        * 새로운 문장의 임베딩벡터와 train의 임베딩 벡터간의 코사인 유사도 계산\n","        * 가장 유사도가 높은 질문의 intent를 찾아 답변 출력하기\n","\n","* 챗봇3개에 대해서 몇가지 질문을 입력하고 각각의 답변을 비교해 봅시다.\n"],"metadata":{"id":"AC6wpFtQtR5y"}},{"cell_type":"markdown","source":["## 1.환경준비"],"metadata":{"id":"pvBAaxSgrkt7"}},{"cell_type":"markdown","source":["### (1)라이브러리 설치"],"metadata":{"id":"AvBsTv3s-a3X"}},{"cell_type":"markdown","source":["#### 1) gensim 3.8.3 설치"],"metadata":{"id":"EdhloaEO-mFZ"}},{"cell_type":"code","source":["#gensim은 자연어 처리를 위한 오픈소스 라이브러리입니다. 토픽 모델링, 워드 임베딩 등 다양한 자연어 처리 기능을 제공\n","# 현재 4.x 버전이 최신이지만, 3.8.3 버전으로 진행\n","!pip install gensim==3.8.3"],"metadata":{"id":"Pkk8tlp2-Q3g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* [코랩] 위 라이브러리 설치후 런타임 재시작 필요!"],"metadata":{"id":"Yvo0_3so-dA8"}},{"cell_type":"markdown","source":["#### 2) 형태소 분석을 위한 라이브러리"],"metadata":{"id":"w9I84wAV-EQ7"}},{"cell_type":"code","source":["# mecab 설치를 위한 관련 패키지 설치\n","!apt-get install curl git\n","!apt-get install build-essential\n","!apt-get install cmake\n","!apt-get install g++\n","!apt-get install flex\n","!apt-get install bison\n","!apt-get install python-dev\n","!pip install cython\n","!pip install mecab-python"],"metadata":{"id":"eRgLSScn0QFG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 형태소 기반 토크나이징 (Konlpy)\n","!python3 -m pip install konlpy\n","# mecab (ubuntu: linux, mac os 기준)\n","# 다른 os 설치 방법 및 자세한 내용은 다음 참고: https://konlpy.org/ko/latest/install/#id1\n","!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"],"metadata":{"id":"7zwNEdPo3Rpm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (2) 라이브러리 불러오기"],"metadata":{"id":"FlRWJB2w6Ip6"}},{"cell_type":"markdown","source":["* 세부 요구사항\n","    - 기본적으로 필요한 라이브러리를 import 하도록 코드가 작성되어 있습니다.\n","    - 필요하다고 판단되는 라이브러리를 추가하세요."],"metadata":{"id":"-TRDCUpP6Ip6"}},{"cell_type":"code","metadata":{"id":"h1IYbPd_6Ip6"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import joblib\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","\n","# 필요하다고 판단되는 라이브러리를 추가하세요.\n","import os\n","\n","from sklearn.model_selection import train_test_split\n","from lightgbm import LGBMClassifier\n","from sklearn.metrics import * \n","\n","import tensorflow as tf\n","from keras.layers import Dense, Embedding, Bidirectional, LSTM, Concatenate, Dropout\n","from keras import Input, Model\n","from keras import optimizers\n","from keras.models import Sequential, load_model\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","from gensim.models import Word2Vec\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 형태소 분석을 위한 함수를 제공합니다."],"metadata":{"id":"ERab2qbnVloB"}},{"cell_type":"code","source":["from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma\n","\n","# 다양한 토크나이저를 사용할 수 있는 함수\n","def get_tokenizer(tokenizer_name):\n","    if tokenizer_name == \"komoran\":\n","        tokenizer = Komoran()\n","    elif tokenizer_name == \"okt\":\n","        tokenizer = Okt()\n","    elif tokenizer_name == \"mecab\":\n","        tokenizer = Mecab()\n","    elif tokenizer_name == \"hannanum\":\n","        tokenizer = Hannanum()\n","    else:\n","        # \"kkma\":\n","        tokenizer = Kkma()\n","        \n","    return tokenizer"],"metadata":{"id":"dGr3phdYVloC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 형태소 분석을 수행하는 함수\n","\n","def tokenize(tokenizer_name, original_sent, nouns=False):\n","    # 미리 정의된 몇 가지 tokenizer 중 하나를 선택\n","    tokenizer = get_tokenizer(tokenizer_name)\n","\n","    # tokenizer를 이용하여 original_sent를 토큰화하여 tokenized_sent에 저장하고, 이를 반환합니다.\n","    sentence = original_sent.replace('\\n', '').strip()\n","    if nouns:       \n","        # tokenizer.nouns(sentence) -> 명사만 추출\n","        tokens = tokenizer.nouns(sentence)\n","    else:\n","        tokens = tokenizer.morphs(sentence)\n","    tokenized_sent = ' '.join(tokens)\n","    \n","    return tokenized_sent"],"metadata":{"id":"f1kGJuX6VloD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (3) 데이터 로딩\n","* 전처리 단계에서 생성한 데이터들을 로딩합니다.\n","    * train, test\n","    * 형태소분석 결과 데이터 : clean_train_questions, clean_test_questions"],"metadata":{"id":"wsLDv9tZc_i1"}},{"cell_type":"markdown","source":["* 구글 드라이브 연결"],"metadata":{"id":"Uc_kIeeJeDgi"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"dd0SPbYdfhS9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = '/content/drive/MyDrive/project/'"],"metadata":{"id":"y5OIDazoeIN4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 저장된 .pkl 파일들을 불러옵니다.\n","* 불러 온 후에는 shape를 확인해 봅시다."],"metadata":{"id":"GH3ApIzofYPb"}},{"cell_type":"code","source":[],"metadata":{"id":"FT_JFnclfcQ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vjQCIzsUsKJD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sY4abSWpW4lb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.챗봇1"],"metadata":{"id":"mMQvj3AmIDEy"}},{"cell_type":"markdown","source":["* **상세요구사항**\n","    * Word2Vec을 활용한 LightGBM 모델링(intent 분류)\n","        * Word2Vec을 이용하여 임베딩벡터 생성하기\n","            * Word Embedding으로 문장벡터 구하기\n","        * 임베딩 벡터를 이용하여 ML기반 모델링 수행하기\n","            * LightGBM 권장(다른 알고리즘을 이용할수 있습니다.)\n","    * 챗봇 : 모델의 예측결과(intent)에 따라 답변하는 챗봇 만들기\n","        * 질문을 입력받아, 답변하는 함수 생성"],"metadata":{"id":"_x9L3XAfJPAh"}},{"cell_type":"markdown","source":["### (1) Word2Vec을 이용하여 임베딩벡터 생성하기\n","* 'mecab' 형태소 분석기를 이용하여 문장을 tokenize\n","    * Word2Vec 모델을 만들기 위해서 입력 데이터는 리스트 형태여야 합니다.\n","    * 그래서 다시 리스트로 저장되도록 토크나이즈 해 봅시다.\n","* Word Embedding으로 문장벡터를 생성합니다.\n","    * 먼저 Word2Vec 모델을 만들고, train의 질문들을 문장벡터로 만듭시다.\n"],"metadata":{"id":"1lQMnaY2SIKM"}},{"cell_type":"markdown","source":["#### 1) 'mecab' 형태소 분석기를 이용하여 문장을 tokenize"],"metadata":{"id":"XZtMR3HVRfCI"}},{"cell_type":"code","source":[],"metadata":{"id":"FHwKdpLvJPc-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YLHDaEeKATKL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2) Word Embedding으로 문장벡터 구하기\n","* Word2Vec\n","    * 위에서 저장한 입력 데이터를 사용하여 Word2Vec 모델이 생성\n","    * 모델은 size(단어 벡터의 차원), \n","    * window(컨텍스트 창의 크기), \n","    * max_vocab_size(고려할 최대 어휘 크기), \n","    * min_count(포함할 단어의 최소 빈도)와 같은 특정 하이퍼파라미터로 훈련됩니다.\n","    * sg : 사용할 훈련 알고리즘 - 1은 skip-gram, 0은 CBOW )"],"metadata":{"id":"ZsDPBh8Nhxy2"}},{"cell_type":"code","source":[],"metadata":{"id":"V6Y26nTvJ1si"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","\n","# Word2Vec 모델 생성\n","wv_model = Word2Vec(                   )"],"metadata":{"id":"s42Yr9cbJ3uf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* Word2Vec 모델로부터 데이터를 벡터화하기 위한 함수 생성"],"metadata":{"id":"PVSSgw6jz-RH"}},{"cell_type":"code","source":["# Word2Vec 모델로부터 하나의 문장을 벡터화 시키는 함수\n","def get_sent_embedding(model, embedding_size, tokenized_words):\n","    # 임베딩 벡터를 0으로 초기화\n","    feature_vec = np.zeros((embedding_size,), dtype='float32')\n","    # 단어 개수 초기화\n","    n_words = 0\n","    # 모델 단어 집합 생성\n","    index2word_set = set(model.wv.index2word)\n","    # 문장의 단어들을 하나씩 반복\n","    for word in tokenized_words:\n","        # 모델 단어 집합에 해당하는 단어일 경우에만\n","        if word in index2word_set:\n","            # 단어 개수 1 증가\n","            n_words += 1\n","            # 임베딩 벡터에 해당 단어의 벡터를 더함\n","            feature_vec = np.add(feature_vec, model[word])\n","    # 단어 개수가 0보다 큰 경우 벡터를 단어 개수로 나눠줌 (평균 임베딩 벡터 계산)\n","    if (n_words > 0):\n","        feature_vec = np.divide(feature_vec, n_words)\n","    return feature_vec"],"metadata":{"id":"vGKxMURH0L0R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 문장벡터 데이터 셋 만들기\n","def get_dataset(sentences, model, num_features):\n","    dataset = list()\n","\n","    # 각 문장을 벡터화해서 리스트에 저장\n","    for sent in sentences:\n","        dataset.append(get_sent_embedding(model, num_features, sent))\n","\n","    # 리스트를 numpy 배열로 변환하여 반환\n","    sent_embedding_vectors = np.stack(dataset)\n","    \n","    return sent_embedding_vectorsR"],"metadata":{"id":"sZS9j5lgKDGQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 이제 학습데이터의 Q를 Word2Vec 모델을 사용하여 벡터화 합니다."],"metadata":{"id":"DEjSR247a9iw"}},{"cell_type":"code","source":["# 학습 데이터의 문장들을 Word2Vec 모델을 사용하여 벡터화\n","train_data_vecs = get_dataset(             )\n"],"metadata":{"id":"bIoehvZna82t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 훈련된 Word2Vec 모델을 사용하여 문장 목록에 대한 문장 임베딩을 생성하고 이를 2차원 numpy 배열에 저장합니다. \n","* 그런 다음 이러한 임베딩을 다양한 기계 학습 모델의 입력 기능으로 사용할 수 있습니다"],"metadata":{"id":"l9XFPz28RmlE"}},{"cell_type":"markdown","source":["### (2) 분류 모델링\n","* 데이터 분할\n","    * x, y\n","        * x : 이전 단계에서 저장된 임베딩벡터(train_data_vecs)\n","        * y : intent 값들\n","    * train, val\n","        * train_test_split 활용\n","* 머신러닝 모델링\n","    * lightGBM, RandomForest 등을 활용하여 학습\n","    * 필요하다면 hyper parameter 튜닝을 시도해도 좋습니다.\n","* validation set으로 검증해 봅시다."],"metadata":{"id":"NOo2RwzWRr0c"}},{"cell_type":"code","source":["# X와 y 데이터 분리\n","\n","\n","# Train-Test split\n","\n"],"metadata":{"id":"vvS1F0EoKTv-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 모델1"],"metadata":{"id":"qKuYKmQ2dbP1"}},{"cell_type":"code","source":[],"metadata":{"id":"MXDMP5gUbQsr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CPPQ3lejdWwq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RCWZOlIlBMAK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 모델2"],"metadata":{"id":"uq63HIFjdc8c"}},{"cell_type":"code","source":[],"metadata":{"id":"4GQPYXMncvkh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"PEXD7iWmDEem"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0ysrB0NyBMVD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (3) 챗봇 구축"],"metadata":{"id":"NxeL02Q1TecY"}},{"cell_type":"markdown","source":["* **상세요구사항**\n","    * 챗봇 flow : input 질문 -> 분류 모델로 intent 예측 --> intent에 해당하는 답변 출력\n","        * 하나의 intent 에는 여러 답변이 있습니다. 이중 한가지를 랜덤하게 선택합니다."],"metadata":{"id":"thqkcWLsTiwc"}},{"cell_type":"markdown","source":["#### 1) 데이터 중 하나에 대해서 테스트"],"metadata":{"id":"GLHnFhw-3dWl"}},{"cell_type":"code","source":[],"metadata":{"id":"8HcLTNykBQHh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"A55XDY5VBQE5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8R-7o3Q9BQB3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"901OF6AvBP-u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Yar1bTRTBP7N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2) 챗봇 함수 만들기\n","* 테스트 코드를 바탕으로 질문을 받아 답변을 하는 함수를 생성합시다.\n","* 성능이 좋은 모델 사용. "],"metadata":{"id":"ZuF2udm93j7W"}},{"cell_type":"code","source":["def get_answer1(question): \n","\n","\n","\n","\n","    return "],"metadata":{"id":"o6YSzGDe3n0M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"oUi-QgPD4HnP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3) test 데이터에 대해서 성능 측정하기\n","\n","test 데이터 전체에 대해서 성능을 측정해 봅시다."],"metadata":{"id":"V1a2FMV5yqwi"}},{"cell_type":"code","source":[],"metadata":{"id":"B8nXCSHX92HI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RWQ_Tv9tBdyO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.챗봇2\n","\n","* **세부요구사항**\n","    * 단계별 챗봇을 만들어 봅시다.\n","        * 1단계 : type을 0과 1로 분류하는 모델 생성(Embedding + LSTM 모델)\n","        * 2단계 : \n","            * 각 type에 맞게, 사전학습된 Word2Vec 모델을 사용하여 임베딩 벡터(train)를 만들고\n","        * 3단계 : 챗봇 만들기\n","            * input 문장과 train 임베딩 벡터와 코사인 유사도 계산\n","            * 가장 유사도가 높은 질문의 intent 찾아\n","            * 해당 intent의 답변 중 무작위로 하나를 선정하여 답변하기"],"metadata":{"id":"6FAB06MnP5qf"}},{"cell_type":"markdown","source":["### (1) 1단계 : type 분류 모델링(LSTM)\n","- LSTM"],"metadata":{"id":"XvAzrVuvVQT9"}},{"cell_type":"markdown","source":["#### 1) 데이터 준비\n","* 학습용 데이터를 만들어 봅시다.\n","    * 시작 데이터 : clean_train_questions, clean_test_questions\n","    * 각 토큰에 인덱스를 부여하는 토크나이저를 만들고 적용\n","        * from tensorflow.keras.preprocessing.text import Tokenizer 를 사용\n","    * 문장별 길이에 대한 분포를 확인하고 적절하게 정의."],"metadata":{"id":"VklTJM3-tuDQ"}},{"cell_type":"code","source":["# 각각의 토큰에 인덱스 부여하는 토크나이저 선언\n","\n","\n","# .fit_on_tests 이용하여 토크나이저 만들기\n","\n"],"metadata":{"id":"TbpMdJT3t228"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 전체 토큰의 수 확인\n","\n"],"metadata":{"id":"ft3qCoGbuIt4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 전체 토큰의 수로 vocab_size 지정\n","\n","\n","# fit_on_texts을 위에서 한번만 해도 되지만, vocab 사이즈를 확인하고 줄이거나 하는 시도를 할 수도 있기에 다시 수행\n","\n","\n","# .texts_to_sequences : 토크나이즈 된 데이터를 가지고 모두 시퀀스로 변환\n","x_train =\n","x_val = "],"metadata":{"id":"mOpw0-EwuJ9b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 각 토큰과 인덱스로 구성된 딕셔너리 생성,\n","\n","\n","# <PAD> 는 0으로 추가\n","\n"],"metadata":{"id":"sVHleK_muYl8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 문장별 토큰수에 대해 탐색적 분석을 수행해 봅시다."],"metadata":{"id":"38FwBFRll05a"}},{"cell_type":"code","source":[],"metadata":{"id":"6hwGM_0dubPR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 문장별 토큰이 가장 큰 것이 57개 입니다. "],"metadata":{"id":"nevMXd0vmj8H"}},{"cell_type":"markdown","source":["* 학습 입력을 위한 데이터 크기 맞추기\n","    * 문장이 짧기 때문에 MAX_SEQUENCE_LENGTH는 정하지 않아도 되지만,\n","    * 그러나 분포를 보고 적절하게 자릅시다.\n","    * 그리고 pad_sequences 함수를 이용하여 시퀀스데이터로 변환하기\n","* y는 train['type'] 와 test['type'] 입니다."],"metadata":{"id":"wQfbaY3HmIMj"}},{"cell_type":"code","source":[],"metadata":{"id":"8M2QWrD-ups7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"6nlnE9dju5Dh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EMlo9o1VngW2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2) 모델링\n","\n","* 토크나이징 한 데이터를 입력으로 받아 \n","* Embedding 레이어와 LSTM 레이어를 결합하여 이진 분류 모델링을 수행합니다."],"metadata":{"id":"WlBeVPA_6ePq"}},{"cell_type":"code","source":[],"metadata":{"id":"8ilRqzlKScHr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0CaxzPmySjte"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"11jhI4eOSlOI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qBeQBXhzSm0y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KOhx2rzGFmiy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (2) 사전학습모델(Word2Vec)"],"metadata":{"id":"NqpAqObGdgbu"}},{"cell_type":"markdown","source":["* Pre-trained Word2Vec model\n","    * 이미 다운로드 받아서 제공되었습니다.\n","        * ko.bin, ko.tsv\n","    * 참고 사이트: https://github.com/Kyubyong/wordvectors\n","        * 모델 파일 다운로드 사이트: https://drive.google.com/file/d/0B0ZXk88koS2KbDhXdWg1Q2RydlU/view?resourcekey=0-Dq9yyzwZxAqT3J02qvnFwg\n","* 사전학습 모델을 로딩하고, \n","* train 데이터셋의 질문(Q)을 임베딩벡터로 만들어, 열(Column)로 추가합니다."],"metadata":{"id":"eJWm6cKwIZjf"}},{"cell_type":"markdown","source":["#### 1) 모델 로딩\n","* 사전 학습된 모델을 로딩 : gensim.models.Word2Vec.load()\n","* 로딩 후 벡터 크기를 조회합시다. .vector_size"],"metadata":{"id":"a5iIl1X57hef"}},{"cell_type":"code","source":["import gensim\n","pre_wv_model = gensim.models.Word2Vec.load(path + 'ko.bin')"],"metadata":{"id":"Wu2_fSn9Vjxx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델의 벡터크기 조회\n","\n"],"metadata":{"id":"Xmq6Fh5lWN0Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2) train 에 임베딩벡터 결과 저장\n","* get_sent_embedding 함수를 이용하여 train의 질문별 임베딩 결과를 저장합니다.\n","    * .apply(lambda .....) 를 활용하세요."],"metadata":{"id":"-reuWNEy7ogd"}},{"cell_type":"code","source":[],"metadata":{"id":"I17jau4YWSD1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Sq3lQ798WVCF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (3) 챗봇 구축\n","* 아래 절차대로 수행하는 함수 만들기\n","    * input 질문 \n","    * 1단계 : 모델을 이용하여 type 0, 1로 분류\n","    * 2단계 : \n","        * train의 모든 Q와 input 문장의 임베딩 벡터간의 코사인 유사도 계산\n","        * 코사인 유사도가 가장 높은 Q를 선택\n","        * 선택한 Q의 intent에 맵핑된 답변 중 하나를 무작위로 선택"],"metadata":{"id":"rilxqf-cX4cN"}},{"cell_type":"markdown","source":["#### 1) 하나의 질문으로 테스트해보기"],"metadata":{"id":"4fUJIN1Sc0YJ"}},{"cell_type":"markdown","source":["* 선택된 질문과 답변"],"metadata":{"id":"8ozGvY19c4xd"}},{"cell_type":"code","source":[],"metadata":{"id":"Zo41eD7Vnd6k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 예측을 위한 입력 형태로 변환\n","    * 학습을 위한 전처리 과정을 test 데이터에도 적용합니다. "],"metadata":{"id":"23bZbPscdA0n"}},{"cell_type":"code","source":[],"metadata":{"id":"qbrsu2szX6xL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 1단계 : type 분류"],"metadata":{"id":"GZzJEnFKdnPe"}},{"cell_type":"code","source":[],"metadata":{"id":"1U6yjYqTp3Nj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 2단계 : 질문에 대한 벡터 만들고 코사인 유사도 계산\n","    * Word2Vec 사전 학습 모델로 부터 벡터 만들기"],"metadata":{"id":"BTC3Rn9Tdtwo"}},{"cell_type":"code","source":[],"metadata":{"id":"dGpWRXtDqNti"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* train의 질문 벡터들과 유사도 계산\n","    * Word2Vec 으로 만든 벡터들과 유사도 계산"],"metadata":{"id":"JOmz_Ib_d6-R"}},{"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","\n","\n","\n"],"metadata":{"id":"EQchLoOJZOPN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2) 챗봇 함수 만들기\n","* 위 테스트 결과를 바탕으로 코드를 정리하고 함수로 생성합니다."],"metadata":{"id":"OPSCaDE5ro5F"}},{"cell_type":"code","source":["def get_answer2(question): \n","\n","\n","\n","\n","\n","    return"],"metadata":{"id":"vK9rO8w6rTRQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7icv4nEt9aLQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3) test 데이터에 대해서 성능 측정하기\n","\n","test 데이터 전체에 대해서 성능을 측정해 봅시다."],"metadata":{"id":"C8cu77Si9rzu"}},{"cell_type":"code","source":[],"metadata":{"id":"EJL_C-Mmtk9f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.챗봇3\n","\n","* **세부요구사항**\n","    * 단계별 챗봇을 만들어 봅시다.\n","        * 1단계 : 챗봇2의 1단계 모델을 사용합니다.\n","        * 2단계 : \n","            * 각 type에 맞게, 사전학습된 Word2Vec 모델을 사용하여 임베딩 벡터(train)를 만들고\n","        * 3단계 : 챗봇 만들기\n","            * input 문장과 train 임베딩 벡터와 코사인 유사도 계산\n","            * 가장 유사도가 높은 질문의 intent 찾아\n","            * 해당 intent의 답변 중 무작위로 하나를 선정하여 답변하기"],"metadata":{"id":"TNn40nu96PI4"}},{"cell_type":"markdown","source":["### (1) 1단계 : type 분류 모델링\n","- LSTM : 3-(1) 모델을 그대로 사용합니다."],"metadata":{"id":"iK7s4ZCt6PI_"}},{"cell_type":"code","source":[],"metadata":{"id":"pOftkHKGAGXk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (2) FastText 모델"],"metadata":{"id":"ebdw6-K9AR_V"}},{"cell_type":"markdown","source":["-  FastText 모델 학습을 위한 입력 포맷 2차원 리스트 형태 입니다.\n","  ```\n","  [['나', '는', '학생', '이다'], ['오늘', '은', '날씨', '가', '좋다']]\n","  ```"],"metadata":{"id":"9RQnEBcnAZNt"}},{"cell_type":"markdown","source":["- Word2Vec계열의 FastText를 학습하는 이유\n","  - n-gram이 추가된 fasttext 모델은 유사한 단어에 대한 임베딩을 word2vec보다 잘 해결할 수 있으며, 오탈자 등에 대한 임베딩 처리가 가능하다.\n","  - 예) 체크카드, 쳌카드는 word2vec에서는 전혀 다른 단어이지만 fasttext는 character n-gram으로 비교적 같은 단어로 처리할 수 있다.\n","- 참고: https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText\n"],"metadata":{"id":"ghuzogkoAZNt"}},{"cell_type":"markdown","source":["#### 1) 데이터 준비\n","* 시작데이터 : clean_train_questions, clean_test_questions"],"metadata":{"id":"HTGtH4bPAh-8"}},{"cell_type":"markdown","source":["* FastText를 위한 입력 데이터 구조 만들기"],"metadata":{"id":"cQIJ4IPZ0b9C"}},{"cell_type":"code","source":[],"metadata":{"id":"Y-kEQiETAZNt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2) FastText 모델 생성\n","* FastText 문법\n","    * FastText( input데이터,  min_count = , size= , window=  )\n","        * input데이터 : 학습에 사용할 문장으로 이루어진 리스트\n","        * min_count : 모델에 사용할 단어의 최소 빈도수. 이 값보다 적게 출현한 단어는 모델에 포함되지 않음. 기본값 = 5\n","        * size : 단어의 벡터 차원 지정. 기본값 = 100\n","        * window : 학습할 때 한 단어의 좌우 몇 개의 단어를 보고 예측을 할 것인지를 지정. 기본값 = 5\n","    * 참조 : https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText"],"metadata":{"id":"46o8NG_LAk7c"}},{"cell_type":"code","source":["from gensim.models.fasttext import FastText\n","import gensim.models.word2vec\n","\n"],"metadata":{"id":"xlWLTr2nAZNu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3) train에 임베딩벡터 결과 저장\n","* get_sent_embedding 함수를 이용하여 train의 질문별 임베딩 결과를 저장합니다.\n","    * .apply(lambda .....) 를 활용하세요."],"metadata":{"id":"NilgkdYPAo0s"}},{"cell_type":"code","source":[],"metadata":{"id":"K2iZx5hRAZNu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (3) 챗봇 구축\n","- input 질문\n","- intent classifier로 common와 faq 중 하나를 예측\n","- 예측된 intent에 속한 train의 모든 Q와 input 문장의 임베딩 벡터간의 코사인 유사도 계산\n","- 코사인 유사도가 가장 높은 top-3개의 Q를 선택\n","- 선택한 Q에 맵핑된 답변 중 하나를 선택하고 실제 답변과 비교"],"metadata":{"id":"HUoQgTf46PJD"}},{"cell_type":"markdown","source":["#### 1) 하나의 질문으로 테스트해보기"],"metadata":{"id":"esu6rcsJ6PJD"}},{"cell_type":"markdown","source":["* 선택된 질문과 답변"],"metadata":{"id":"dCxYTX2W6PJD"}},{"cell_type":"code","source":[],"metadata":{"id":"9Nl6O3Di6PJD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 예측을 위한 입력 형태로 변환"],"metadata":{"id":"g-8Ck-Mh6PJD"}},{"cell_type":"code","source":[],"metadata":{"id":"j0yGO9yQ6PJE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 예측하기"],"metadata":{"id":"-yp2MBmo6PJE"}},{"cell_type":"code","source":[],"metadata":{"id":"BYm1AODK6PJE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 질문에 대한 벡터 만들기\n","    * FestText 모델로 부터 벡터 만들기"],"metadata":{"id":"uF0D3SH16PJE"}},{"cell_type":"code","source":[],"metadata":{"id":"i_6k-Tjf6PJE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* train의 질문 벡터들과 유사도 계산\n","    * FastText 로 만들 벡터들과 유사도 계산"],"metadata":{"id":"rgEVj1WG6PJE"}},{"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","\n","\n"],"metadata":{"id":"OSX-j8WV6PJE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","#### 2) 함수로 생성하기"],"metadata":{"id":"q-IJcMH26PJF"}},{"cell_type":"code","source":["def get_answer3(question): \n","\n","\n","\n","\n","\n","\n","\n","    return "],"metadata":{"id":"dYlaEeNN6PJF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gONYObLpBwgt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 3) test 데이터에 대해서 성능 측정하기\n","\n","test 데이터 전체에 대해서 성능을 측정해 봅시다."],"metadata":{"id":"GNfo5m5Hyswe"}},{"cell_type":"code","source":[],"metadata":{"id":"IKFBp5LMBwgt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"fF1JMFRs6PJG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5.질문에 대한 답변 비교해보기\n","\n","* **세부요구사항**\n","    * 세가지 챗봇을 생성해 보았습니다. \n","    * 질문을 입력하여 답변을 비교해 봅시다. 어떤 챗봇이 좀 더 정확한 답변을 하나요?\n"],"metadata":{"id":"BEjQHVAJCKkN"}},{"cell_type":"code","source":[],"metadata":{"id":"c8q4Vu5CCvSP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-i0609ZGDYYd"},"execution_count":null,"outputs":[]}]}